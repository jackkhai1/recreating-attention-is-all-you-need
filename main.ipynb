{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# some notes for learning\n",
    "# basically this code provides the class for the components within the transformer and covers a few key details\n",
    "# the first step is inheriting some variable from the parent class , these are embed size and head\n",
    "# embed size refers to the vector representation of the embedded token\n",
    "# head refers to the number of attention heads that are split within the sentence or corpus of text\n",
    "# head_dim refers to the embed_size/ head , this value must be divisible. \n",
    "\n",
    "\n",
    "\n",
    "## the following texts will describe a single head of attention or self attention head\n",
    "\n",
    "# keys, values and queries are important for the attention mechanisms\n",
    "# query is like each token asking \"which token is important for me to pay attention to?\"\n",
    "# the key represents each tokens \"identity card\", holding information about itself that it can share with others\n",
    "# the value holds teh actual content or information each token has to offer once the model decides to focus on it. \n",
    "\n",
    "# to create these queries, keys and values vectors, we have to pass each embedded vector for the tokens through three separate linear transformations. the next stage is creating linear transformations for the key, queries and values\n",
    "# each of these transformations takes the embedding vector and outputs a new vector for each word. \n",
    "\n",
    "\n",
    "# as an explanation difference between embeddings and the attention mechanism, imagine you have the word \"mole\" that has a different context depending on the sentence or the words surrounding it\n",
    "# the embeddings vector representation would be the same across the different instances of the word or token\n",
    "# however, attention is able to pull information from around the word using keys, queries and value calculations to shift the embedded vector representation into a new direction such that it can learn the new representations.\n",
    "\n",
    "# for the queries and keys, each query and key will be multiplied via dot multiplication is a matrix and the \"size\" will be calculated. This value shows the tokens that align between the keys and the queries. \n",
    "# you want the multiplied values to have a probability distribution, and therefore have to have a min of 0 and max of 1. This is done using softmax to normalise.\n",
    "\n",
    "# masking is used where tokens are allowed to attend only to earlier positions in the sequence but not later ones, therefore, the later tokens are masked out. This is done by setting later tokens a very low attention score. \n",
    "# the reason why masking is used after dot product multiplication is because you want the queries and keys for the heads to learn the future context first before it is masked out \n",
    "# the reason for masking is for training, so that the model does not cheat and use future tokens to check on its accuracy. \n",
    "\n",
    "\n",
    "# the value matrix: If you have 2 related words from the query key matrix, for example \"fluffy creature\", you multiply the first embedding vector by the value matrix to get a value vector. \n",
    "# this value vector is added to the embedding of the second word to shift the embedding into the high dimensional space given the \"fluffy\" context. \n",
    "# thats a simplification, in reality, there are multiple keys that can influence the token you are attending to. Therefore, all the relevant queries key matrices that have a high value are multiplied by the value vector and added up\n",
    "# the ones that have a low probability of being associated with the specific token youre looking at will have a probability of 0 \n",
    "# the vectors are added up and the added value is the \"direction\" that the new head is going towards. \n",
    "\n",
    "\n",
    "# these heads of attention are then run in parallel into \"multi headed attention\" with its own distinct keys queries and values. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.head = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "\n",
    "        assert(self.head_dim * heads == embed_size), \"Embed size needs to be div by heads\"\n",
    "\n",
    "        self.values = nn.linear(self.head_dim, self.head_dim, bias = False)\n",
    "        self.keys = nn.linear(self.head_dim, self.head_dim, bias = False)\n",
    "        self.queries = nn.linear(self.head_dim, self.head_dim, bias = False)\n",
    "        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
    "\n",
    "    def forward(self, values, keys, query, mask ):\n",
    "        N = query.shape[0]\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "\n",
    "        #split embedding into self.heads pieces\n",
    "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
    "        queries = query.reshape(N, key_len, self.heads, self.head_dim)\n",
    "\n",
    "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "        attention = torch.softmax((energy/(self.embed_size) ** (1/2)), dim = 3)\n",
    "\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
    "            N, query_len, self.heads * self.head_dim \n",
    "        )\n",
    "\n",
    "        out = self.fc_out(out)\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, dropout, forward_expansion):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = SelfAttention(embed_size, heads)\n",
    "\n",
    "        self.norm1 = nn."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jackkhai",
   "language": "python",
   "name": "jackkhai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
